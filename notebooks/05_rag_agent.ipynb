{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. RAG Agent Implementation\n",
    "\n",
    "This notebook implements and tests the Retrieval-Augmented Generation (RAG) system for ESG fraud detection.\n",
    "\n",
    "## Objectives\n",
    "- Set up RAG system with ESG corpora\n",
    "- Implement document processing and vectorization\n",
    "- Test RAG-based analysis\n",
    "- Evaluate RAG performance\n",
    "- Integrate with AI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import RAG utilities\n",
    "from rag_utils import (\n",
    "    DocumentProcessor, \n",
    "    VectorStore, \n",
    "    RAGAnalyzer, \n",
    "    ESGCorporaIngester,\n",
    "    create_esg_corpora\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_parquet('../data/clean_claims.parquet')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Greenwashing rate: {df['greenwashing_flag'].mean():.2%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ESG Corpora Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ESG corpora\n",
    "print(\"=== ESG CORPORA SETUP ===\\n\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../data/corpora', exist_ok=True)\n",
    "os.makedirs('../data/vector_stores', exist_ok=True)\n",
    "\n",
    "# Create ESG corpora\n",
    "print(\"Creating ESG corpora...\")\n",
    "esg_corpora = create_esg_corpora()\n",
    "\n",
    "print(f\"Created {len(esg_corpora)} ESG corpora:\")\n",
    "for corpus_name, corpus_data in esg_corpora.items():\n",
    "    print(f\"  - {corpus_name}: {len(corpus_data)} documents\")\n",
    "\n",
    "# Save corpora\n",
    "for corpus_name, corpus_data in esg_corpora.items():\n",
    "    corpus_file = f'../data/corpora/{corpus_name}.json'\n",
    "    with open(corpus_file, 'w') as f:\n",
    "        import json\n",
    "        json.dump(corpus_data, f, indent=2)\n",
    "    print(f\"Saved: {corpus_file}\")\n",
    "\n",
    "# Display sample documents\n",
    "print(\"\\nSample documents from ESG Standards:\")\n",
    "for i, doc in enumerate(esg_corpora['esg_standards'][:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Content: {doc['content'][:200]}...\")\n",
    "    print(f\"Tags: {doc['tags']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document processor\n",
    "print(\"=== DOCUMENT PROCESSING ===\\n\")\n",
    "\n",
    "processor = DocumentProcessor()\n",
    "print(\"Document processor initialized\")\n",
    "\n",
    "# Process ESG corpora\n",
    "processed_docs = {}\n",
    "for corpus_name, corpus_data in esg_corpora.items():\n",
    "    print(f\"\\nProcessing {corpus_name}...\")\n",
    "    processed = processor.process_documents(corpus_data)\n",
    "    processed_docs[corpus_name] = processed\n",
    "    print(f\"  Processed {len(processed)} documents\")\n",
    "    \n",
    "    # Show sample processed document\n",
    "    if processed:\n",
    "        sample_doc = processed[0]\n",
    "        print(f\"  Sample processed document:\")\n",
    "        print(f\"    Title: {sample_doc['title']}\")\n",
    "        print(f\"    Chunks: {len(sample_doc['chunks'])}\")\n",
    "        print(f\"    First chunk: {sample_doc['chunks'][0][:100]}...\")\n",
    "\n",
    "# Save processed documents\n",
    "for corpus_name, processed in processed_docs.items():\n",
    "    processed_file = f'../data/corpora/{corpus_name}_processed.json'\n",
    "    with open(processed_file, 'w') as f:\n",
    "        import json\n",
    "        json.dump(processed, f, indent=2)\n",
    "    print(f\"Saved: {processed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "print(\"=== VECTOR STORE SETUP ===\\n\")\n",
    "\n",
    "vector_store = VectorStore()\n",
    "print(\"Vector store initialized\")\n",
    "\n",
    "# Add documents to vector store\n",
    "total_docs = 0\n",
    "for corpus_name, processed in processed_docs.items():\n",
    "    print(f\"\\nAdding {corpus_name} to vector store...\")\n",
    "    \n",
    "    # Flatten chunks from all documents\n",
    "    all_chunks = []\n",
    "    for doc in processed:\n",
    "        for i, chunk in enumerate(doc['chunks']):\n",
    "            all_chunks.append({\n",
    "                'content': chunk,\n",
    "                'metadata': {\n",
    "                    'title': doc['title'],\n",
    "                    'corpus': corpus_name,\n",
    "                    'tags': doc['tags'],\n",
    "                    'chunk_id': i\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Add to vector store\n",
    "    vector_store.add_documents(all_chunks)\n",
    "    total_docs += len(all_chunks)\n",
    "    print(f\"  Added {len(all_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal chunks in vector store: {total_docs}\")\n",
    "\n",
    "# Save vector store\n",
    "vector_store.save('../data/vector_stores/esg_vector_store')\n",
    "print(\"Vector store saved to ../data/vector_stores/esg_vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Analyzer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG analyzer\n",
    "print(\"=== RAG ANALYZER SETUP ===\\n\")\n",
    "\n",
    "rag_analyzer = RAGAnalyzer(vector_store)\n",
    "print(\"RAG analyzer initialized\")\n",
    "\n",
    "# Test RAG analyzer with sample claims\n",
    "print(\"\\nTesting RAG analyzer with sample claims...\")\n",
    "\n",
    "# Get sample claims from dataset\n",
    "sample_claims = df[df['greenwashing_flag'] == 1]['esg_claim_text'].head(3).tolist()\n",
    "sample_claims.extend(df[df['greenwashing_flag'] == 0]['esg_claim_text'].head(2).tolist())\n",
    "\n",
    "for i, claim in enumerate(sample_claims):\n",
    "    print(f\"\\nTest Claim {i+1}: {claim[:100]}...\")\n",
    "    \n",
    "    # Analyze claim\n",
    "    analysis = rag_analyzer.analyze_claim(claim)\n",
    "    \n",
    "    print(f\"  Risk Score: {analysis['risk_score']:.2f}\")\n",
    "    print(f\"  Compliance Score: {analysis['compliance_score']:.2f}\")\n",
    "    print(f\"  Key Issues: {analysis['key_issues'][:3]}\")\n",
    "    print(f\"  Relevant Standards: {analysis['relevant_standards'][:2]}\")\n",
    "    print(f\"  Recommendations: {analysis['recommendations'][:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG performance on test set\n",
    "print(\"=== RAG PERFORMANCE EVALUATION ===\\n\")\n",
    "\n",
    "# Create test set\n",
    "test_claims = df.sample(n=min(50, len(df)), random_state=42)\n",
    "print(f\"Test set size: {len(test_claims)}\")\n",
    "\n",
    "# Analyze test claims\n",
    "rag_results = []\n",
    "for idx, row in test_claims.iterrows():\n",
    "    claim = row['esg_claim_text']\n",
    "    actual_greenwashing = row['greenwashing_flag']\n",
    "    \n",
    "    # Get RAG analysis\n",
    "    analysis = rag_analyzer.analyze_claim(claim)\n",
    "    \n",
    "    # Determine prediction based on risk score\n",
    "    predicted_greenwashing = 1 if analysis['risk_score'] > 0.5 else 0\n",
    "    \n",
    "    rag_results.append({\n",
    "        'claim': claim,\n",
    "        'actual': actual_greenwashing,\n",
    "        'predicted': predicted_greenwashing,\n",
    "        'risk_score': analysis['risk_score'],\n",
    "        'compliance_score': analysis['compliance_score'],\n",
    "        'key_issues': analysis['key_issues'],\n",
    "        'relevant_standards': analysis['relevant_standards']\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "actual = [r['actual'] for r in rag_results]\n",
    "predicted = [r['predicted'] for r in rag_results]\n",
    "risk_scores = [r['risk_score'] for r in rag_results]\n",
    "\n",
    "rag_metrics = {\n",
    "    'accuracy': accuracy_score(actual, predicted),\n",
    "    'precision': precision_score(actual, predicted),\n",
    "    'recall': recall_score(actual, predicted),\n",
    "    'f1_score': f1_score(actual, predicted),\n",
    "    'roc_auc': roc_auc_score(actual, risk_scores)\n",
    "}\n",
    "\n",
    "print(\"RAG Performance Metrics:\")\n",
    "for metric, value in rag_metrics.items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Risk score distribution by actual label\n",
    "actual_0_scores = [r['risk_score'] for r in rag_results if r['actual'] == 0]\n",
    "actual_1_scores = [r['risk_score'] for r in rag_results if r['actual'] == 1]\n",
    "\n",
    "axes[0].hist(actual_0_scores, bins=15, alpha=0.7, label='Legitimate', color='green')\n",
    "axes[0].hist(actual_1_scores, bins=15, alpha=0.7, label='Greenwashing', color='red')\n",
    "axes[0].set_title('Risk Score Distribution by Actual Label')\n",
    "axes[0].set_xlabel('Risk Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Compliance score vs Risk score\n",
    "compliance_scores = [r['compliance_score'] for r in rag_results]\n",
    "colors = ['red' if r['actual'] == 1 else 'green' for r in rag_results]\n",
    "\n",
    "axes[1].scatter(risk_scores, compliance_scores, c=colors, alpha=0.7)\n",
    "axes[1].set_title('Compliance Score vs Risk Score')\n",
    "axes[1].set_xlabel('Risk Score')\n",
    "axes[1].set_ylabel('Compliance Score')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/rag_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AI Agent Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AI agent integration\n",
    "print(\"=== AI AGENT INTEGRATION ===\\n\")\n",
    "\n",
    "# Import agent runner\n",
    "from agent_runner import ESGAgentRunner\n",
    "\n",
    "# Initialize agent\n",
    "agent = ESGAgentRunner()\n",
    "print(\"AI agent initialized\")\n",
    "\n",
    "# Test agent with sample claims\n",
    "print(\"\\nTesting AI agent with sample claims...\")\n",
    "\n",
    "test_claims = [\n",
    "    \"Our company has achieved 100% carbon neutrality through innovative renewable energy solutions.\",\n",
    "    \"We are committed to sustainable practices and environmental stewardship.\",\n",
    "    \"Our ESG initiatives have resulted in a 50% reduction in emissions while maintaining profitability.\"\n",
    "]\n",
    "\n",
    "for i, claim in enumerate(test_claims):\n",
    "    print(f\"\\nTest Claim {i+1}: {claim}\")\n",
    "    \n",
    "    # Analyze with agent\n",
    "    result = agent.analyze_claim(claim)\n",
    "    \n",
    "    print(f\"  Overall Risk Score: {result['overall_risk_score']:.2f}\")\n",
    "    print(f\"  Classification: {result['classification']}\")\n",
    "    print(f\"  Greenwashing Probability: {result['greenwashing_probability']:.2f}\")\n",
    "    print(f\"  Key Risk Factors: {result['key_risk_factors'][:3]}\")\n",
    "    print(f\"  Recommendations: {result['recommendations'][:2]}\")\n",
    "    print(f\"  Regulatory Compliance: {result['regulatory_compliance']}\")\n",
    "\n",
    "# Test batch analysis\n",
    "print(\"\\nTesting batch analysis...\")\n",
    "batch_results = agent.analyze_batch(test_claims)\n",
    "print(f\"Batch analysis completed for {len(batch_results)} claims\")\n",
    "\n",
    "# Show batch summary\n",
    "risk_scores = [r['overall_risk_score'] for r in batch_results]\n",
    "greenwashing_probs = [r['greenwashing_probability'] for r in batch_results]\n",
    "\n",
    "print(f\"\\nBatch Summary:\")\n",
    "print(f\"  Average Risk Score: {np.mean(risk_scores):.2f}\")\n",
    "print(f\"  Average Greenwashing Probability: {np.mean(greenwashing_probs):.2f}\")\n",
    "print(f\"  High Risk Claims: {sum(1 for r in risk_scores if r > 0.7)}\")\n",
    "print(f\"  Likely Greenwashing: {sum(1 for p in greenwashing_probs if p > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RAG vs Traditional Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG with traditional models\n",
    "print(\"=== RAG VS TRADITIONAL MODELS ===\\n\")\n",
    "\n",
    "# Load traditional models\n",
    "import pickle\n",
    "\n",
    "# Load baseline model\n",
    "with open('../models/greenwashing_classifier.pkl', 'rb') as f:\n",
    "    baseline_model = pickle.load(f)\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "with open('../models/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Load advanced model\n",
    "with open('../models/sentence_transformer.pkl', 'rb') as f:\n",
    "    sentence_model = pickle.load(f)\n",
    "\n",
    "# Compare on test set\n",
    "comparison_results = []\n",
    "\n",
    "for i, row in test_claims.iterrows():\n",
    "    claim = row['esg_claim_text']\n",
    "    actual = row['greenwashing_flag']\n",
    "    \n",
    "    # Baseline model prediction\n",
    "    baseline_features = tfidf_vectorizer.transform([claim])\n",
    "    baseline_pred = baseline_model.predict(baseline_features)[0]\n",
    "    baseline_proba = baseline_model.predict_proba(baseline_features)[0][1]\n",
    "    \n",
    "    # RAG prediction\n",
    "    rag_analysis = rag_analyzer.analyze_claim(claim)\n",
    "    rag_pred = 1 if rag_analysis['risk_score'] > 0.5 else 0\n",
    "    rag_proba = rag_analysis['risk_score']\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'claim': claim,\n",
    "        'actual': actual,\n",
    "        'baseline_pred': baseline_pred,\n",
    "        'baseline_proba': baseline_proba,\n",
    "        'rag_pred': rag_pred,\n",
    "        'rag_proba': rag_proba\n",
    "    })\n",
    "\n",
    "# Calculate comparison metrics\n",
    "actual = [r['actual'] for r in comparison_results]\n",
    "baseline_pred = [r['baseline_pred'] for r in comparison_results]\n",
    "rag_pred = [r['rag_pred'] for r in comparison_results]\n",
    "baseline_proba = [r['baseline_proba'] for r in comparison_results]\n",
    "rag_proba = [r['rag_proba'] for r in comparison_results]\n",
    "\n",
    "comparison_metrics = {\n",
    "    'baseline': {\n",
    "        'accuracy': accuracy_score(actual, baseline_pred),\n",
    "        'precision': precision_score(actual, baseline_pred),\n",
    "        'recall': recall_score(actual, baseline_pred),\n",
    "        'f1_score': f1_score(actual, baseline_pred),\n",
    "        'roc_auc': roc_auc_score(actual, baseline_proba)\n",
    "    },\n",
    "    'rag': {\n",
    "        'accuracy': accuracy_score(actual, rag_pred),\n",
    "        'precision': precision_score(actual, rag_pred),\n",
    "        'recall': recall_score(actual, rag_pred),\n",
    "        'f1_score': f1_score(actual, rag_pred),\n",
    "        'roc_auc': roc_auc_score(actual, rag_proba)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"\\nBaseline Model (TF-IDF + Logistic Regression):\")\n",
    "for metric, value in comparison_metrics['baseline'].items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nRAG Model:\")\n",
    "for metric, value in comparison_metrics['rag'].items():\n",
    "    print(f\"  {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "baseline_values = [comparison_metrics['baseline'][metric] for metric in metrics]\n",
    "rag_values = [comparison_metrics['rag'][metric] for metric in metrics]\n",
    "\n",
    "ax.bar(x - width/2, baseline_values, width, label='Baseline Model', color='skyblue')\n",
    "ax.bar(x + width/2, rag_values, width, label='RAG Model', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('RAG vs Baseline Model Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/rag_vs_baseline.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save RAG System and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RAG system and results\n",
    "print(\"=== SAVING RAG SYSTEM ===\\n\")\n",
    "\n",
    "# Save RAG analyzer\n",
    "with open('../models/rag_analyzer.pkl', 'wb') as f:\n",
    "    pickle.dump(rag_analyzer, f)\n",
    "print(\"Saved: rag_analyzer.pkl\")\n",
    "\n",
    "# Save AI agent\n",
    "with open('../models/ai_agent.pkl', 'wb') as f:\n",
    "    pickle.dump(agent, f)\n",
    "print(\"Saved: ai_agent.pkl\")\n",
    "\n",
    "# Save RAG metrics\n",
    "print(\"\\n=== SAVING RAG METRICS ===\\n\")\n",
    "\n",
    "rag_system_metrics = {\n",
    "    'rag_performance': rag_metrics,\n",
    "    'model_comparison': comparison_metrics,\n",
    "    'system_info': {\n",
    "        'total_corpora': len(esg_corpora),\n",
    "        'total_documents': sum(len(corpus) for corpus in esg_corpora.values()),\n",
    "        'total_chunks': total_docs,\n",
    "        'vector_store_size': total_docs,\n",
    "        'test_set_size': len(test_claims)\n",
    "    },\n",
    "    'corpora_info': {\n",
    "        name: {\n",
    "            'documents': len(corpus),\n",
    "            'processed_chunks': len(processed_docs[name])\n",
    "        } for name, corpus in esg_corpora.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../metrics/rag_system_metrics.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(rag_system_metrics, f, indent=2)\n",
    "print(\"Saved: rag_system_metrics.json\")\n",
    "\n",
    "# Save detailed RAG results\n",
    "with open('../metrics/rag_detailed_results.json', 'w') as f:\n",
    "    json.dump(rag_results, f, indent=2)\n",
    "print(\"Saved: rag_detailed_results.json\")\n",
    "\n",
    "# Save comparison results\n",
    "with open('../metrics/model_comparison_detailed.json', 'w') as f:\n",
    "    json.dump(comparison_results, f, indent=2)\n",
    "print(\"Saved: model_comparison_detailed.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RAG AGENT SUMMARY ===\\n\")\n",
    "\n",
    "print(\"1. ESG CORPORA:\")\n",
    "print(f\"   - Total corpora: {rag_system_metrics['system_info']['total_corpora']}\")\n",
    "print(f\"   - Total documents: {rag_system_metrics['system_info']['total_documents']}\")\n",
    "print(f\"   - Total chunks: {rag_system_metrics['system_info']['total_chunks']}\")\n",
    "\n",
    "print(\"\\n2. RAG PERFORMANCE:\")\n",
    "for metric, value in rag_metrics.items():\n",
    "    print(f\"   {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n3. MODEL COMPARISON:\")\n",
    "print(\"   Baseline Model:\")\n",
    "for metric, value in comparison_metrics['baseline'].items():\n",
    "    print(f\"     {metric.capitalize()}: {value:.3f}\")\n",
    "print(\"   RAG Model:\")\n",
    "for metric, value in comparison_metrics['rag'].items():\n",
    "    print(f\"     {metric.capitalize()}: {value:.3f}\")\n",
    "\n",
    "print(\"\\n4. AI AGENT:\")\n",
    "print(\"   - Successfully integrated with RAG system\")\n",
    "print(\"   - Provides comprehensive analysis with risk scores\")\n",
    "print(\"   - Supports both single claim and batch analysis\")\n",
    "print(\"   - Includes regulatory compliance assessment\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS:\")\n",
    "print(\"   - RAG system provides interpretable results with regulatory context\")\n",
    "print(\"   - AI agent combines multiple analysis approaches\")\n",
    "print(\"   - System can identify specific compliance issues\")\n",
    "print(\"   - Provides actionable recommendations\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS:\")\n",
    "print(\"   - RAG system ready for deployment\")\n",
    "print(\"   - AI agent can be integrated into Streamlit app\")\n",
    "print(\"   - Consider expanding ESG corpora with more recent regulations\")\n",
    "print(\"   - Proceed to business plan generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 