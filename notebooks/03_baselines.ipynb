{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Baseline Models\n",
    "\n",
    "This notebook implements and evaluates baseline machine learning models for ESG greenwashing detection.\n",
    "\n",
    "## Objectives\n",
    "- Implement TF-IDF + Logistic Regression baseline\n",
    "- Train models for claim category classification\n",
    "- Train models for greenwashing detection\n",
    "- Evaluate model performance\n",
    "- Save baseline models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_parquet('../data/clean_claims.parquet')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Greenwashing rate: {df['greenwashing_flag'].mean():.2%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"=== DATA PREPARATION ===\\n\")\n",
    "\n",
    "# Check for missing values in target variables\n",
    "print(\"Missing values in target variables:\")\n",
    "print(f\"claim_category: {df['claim_category'].isnull().sum()}\")\n",
    "print(f\"greenwashing_flag: {df['greenwashing_flag'].isnull().sum()}\")\n",
    "\n",
    "# Remove rows with missing targets\n",
    "df_model = df.dropna(subset=['claim_category', 'greenwashing_flag', 'esg_claim_text'])\n",
    "print(f\"\\nRows after removing missing targets: {len(df_model)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"Claim Categories:\")\n",
    "print(df_model['claim_category'].value_counts())\n",
    "print(\"\\nGreenwashing Flag:\")\n",
    "print(df_model['greenwashing_flag'].value_counts())\n",
    "print(f\"Greenwashing rate: {df_model['greenwashing_flag'].mean():.2%}\")\n",
    "\n",
    "# Prepare features and targets\n",
    "X_text = df_model['esg_claim_text'].values\n",
    "y_category = df_model['claim_category'].values\n",
    "y_greenwashing = df_model['greenwashing_flag'].values\n",
    "\n",
    "print(f\"\\nFeature shape: {X_text.shape}\")\n",
    "print(f\"Category target shape: {y_category.shape}\")\n",
    "print(f\"Greenwashing target shape: {y_greenwashing.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "print(\"=== TF-IDF VECTORIZATION ===\\n\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_tfidf = tfidf.fit_transform(X_text)\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Show some feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nSample feature names: {feature_names[:20]}\")\n",
    "\n",
    "# Analyze feature importance\n",
    "tfidf_sums = X_tfidf.sum(axis=0).A1\n",
    "top_features_idx = tfidf_sums.argsort()[-20:][::-1]\n",
    "top_features = [feature_names[i] for i in top_features_idx]\n",
    "top_scores = [tfidf_sums[i] for i in top_features_idx]\n",
    "\n",
    "print(f\"\\nTop 20 most frequent terms:\")\n",
    "for term, score in zip(top_features, top_scores):\n",
    "    print(f\"{term}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Claim Category Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for category classification\n",
    "print(\"=== CLAIM CATEGORY CLASSIFICATION ===\\n\")\n",
    "\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_tfidf, y_category, test_size=0.2, random_state=42, stratify=y_category\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_cat.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_cat.shape[0]} samples\")\n",
    "\n",
    "# Train Logistic Regression for category classification\n",
    "lr_category = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_category.fit(X_train_cat, y_train_cat)\n",
    "\n",
    "# Predictions\n",
    "y_pred_cat = lr_category.predict(X_test_cat)\n",
    "y_pred_proba_cat = lr_category.predict_proba(X_test_cat)\n",
    "\n",
    "# Evaluate category classification\n",
    "print(\"\\nCategory Classification Results:\")\n",
    "print(classification_report(y_test_cat, y_pred_cat))\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores_cat = cross_val_score(lr_category, X_tfidf, y_category, cv=5, scoring='accuracy')\n",
    "print(f\"\\nCross-validation accuracy: {cv_scores_cat.mean():.3f} (+/- {cv_scores_cat.std() * 2:.3f})\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_cat = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_cat, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=lr_category.classes_, \n",
    "            yticklabels=lr_category.classes_)\n",
    "plt.title('Confusion Matrix - Category Classification')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/category_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Greenwashing Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for greenwashing detection\n",
    "print(\"=== GREENWASHING DETECTION ===\\n\")\n",
    "\n",
    "X_train_gw, X_test_gw, y_train_gw, y_test_gw = train_test_split(\n",
    "    X_tfidf, y_greenwashing, test_size=0.2, random_state=42, stratify=y_greenwashing\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_gw.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_gw.shape[0]} samples\")\n",
    "print(f\"Training greenwashing rate: {y_train_gw.mean():.2%}\")\n",
    "print(f\"Test greenwashing rate: {y_test_gw.mean():.2%}\")\n",
    "\n",
    "# Train Logistic Regression for greenwashing detection\n",
    "lr_greenwashing = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "lr_greenwashing.fit(X_train_gw, y_train_gw)\n",
    "\n",
    "# Predictions\n",
    "y_pred_gw = lr_greenwashing.predict(X_test_gw)\n",
    "y_pred_proba_gw = lr_greenwashing.predict_proba(X_test_gw)[:, 1]\n",
    "\n",
    "# Evaluate greenwashing detection\n",
    "print(\"\\nGreenwashing Detection Results:\")\n",
    "print(classification_report(y_test_gw, y_pred_gw))\n",
    "\n",
    "# ROC-AUC score\n",
    "roc_auc_gw = roc_auc_score(y_test_gw, y_pred_proba_gw)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_gw:.3f}\")\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores_gw = cross_val_score(lr_greenwashing, X_tfidf, y_greenwashing, cv=5, scoring='roc_auc')\n",
    "print(f\"Cross-validation ROC-AUC: {cv_scores_gw.mean():.3f} (+/- {cv_scores_gw.std() * 2:.3f})\")\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test_gw, y_pred_proba_gw)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {roc_auc_gw:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Greenwashing Detection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/greenwashing_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm_gw = confusion_matrix(y_test_gw, y_pred_gw)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_gw, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=['Legitimate', 'Greenwashing'], \n",
    "            yticklabels=['Legitimate', 'Greenwashing'])\n",
    "plt.title('Confusion Matrix - Greenwashing Detection')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/greenwashing_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for greenwashing detection\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\\n\")\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = lr_greenwashing.coef_[0]\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get top positive and negative features\n",
    "top_positive_idx = feature_importance.argsort()[-20:][::-1]\n",
    "top_negative_idx = feature_importance.argsort()[:20]\n",
    "\n",
    "print(\"Top 20 features associated with Greenwashing:\")\n",
    "for idx in top_positive_idx:\n",
    "    print(f\"{feature_names[idx]}: {feature_importance[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nTop 20 features associated with Legitimate Claims:\")\n",
    "for idx in top_negative_idx:\n",
    "    print(f\"{feature_names[idx]}: {feature_importance[idx]:.3f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Top positive features\n",
    "top_positive_features = [feature_names[i] for i in top_positive_idx]\n",
    "top_positive_scores = [feature_importance[i] for i in top_positive_idx]\n",
    "\n",
    "axes[0].barh(range(len(top_positive_features)), top_positive_scores, color='red')\n",
    "axes[0].set_yticks(range(len(top_positive_features)))\n",
    "axes[0].set_yticklabels(top_positive_features)\n",
    "axes[0].set_title('Top Features Associated with Greenwashing')\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "\n",
    "# Top negative features\n",
    "top_negative_features = [feature_names[i] for i in top_negative_idx]\n",
    "top_negative_scores = [feature_importance[i] for i in top_negative_idx]\n",
    "\n",
    "axes[1].barh(range(len(top_negative_features)), top_negative_scores, color='green')\n",
    "axes[1].set_yticks(range(len(top_negative_features)))\n",
    "axes[1].set_yticklabels(top_negative_features)\n",
    "axes[1].set_title('Top Features Associated with Legitimate Claims')\n",
    "axes[1].set_xlabel('Coefficient Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models for greenwashing detection\n",
    "print(\"=== MODEL COMPARISON ===\\n\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_gw, y_train_gw)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_gw)\n",
    "    y_pred_proba = model.predict_proba(X_test_gw)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test_gw, y_pred),\n",
    "        'precision': precision_score(y_test_gw, y_pred),\n",
    "        'recall': recall_score(y_test_gw, y_pred),\n",
    "        'f1': f1_score(y_test_gw, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test_gw, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Accuracy: {results[name]['accuracy']:.3f}\")\n",
    "    print(f\"  Precision: {results[name]['precision']:.3f}\")\n",
    "    print(f\"  Recall: {results[name]['recall']:.3f}\")\n",
    "    print(f\"  F1-Score: {results[name]['f1']:.3f}\")\n",
    "    print(f\"  ROC-AUC: {results[name]['roc_auc']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize model comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    values = [result[metric] for metric in metrics]\n",
    "    ax.bar(x + i*width, values, width, label=name)\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison')\n",
    "ax.set_xticks(x + width/2)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../metrics', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "print(\"=== SAVING MODELS ===\\n\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "with open('../models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "print(\"Saved: tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save category classification model\n",
    "with open('../models/category_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_category, f)\n",
    "print(\"Saved: category_classifier.pkl\")\n",
    "\n",
    "# Save greenwashing detection model\n",
    "with open('../models/greenwashing_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_greenwashing, f)\n",
    "print(\"Saved: greenwashing_classifier.pkl\")\n",
    "\n",
    "# Save Random Forest model\n",
    "with open('../models/random_forest_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(models['Random Forest'], f)\n",
    "print(\"Saved: random_forest_classifier.pkl\")\n",
    "\n",
    "# Save metrics\n",
    "print(\"\\n=== SAVING METRICS ===\\n\")\n",
    "\n",
    "# Category classification metrics\n",
    "category_metrics = {\n",
    "    'accuracy': (y_pred_cat == y_test_cat).mean(),\n",
    "    'cv_accuracy_mean': cv_scores_cat.mean(),\n",
    "    'cv_accuracy_std': cv_scores_cat.std(),\n",
    "    'classification_report': classification_report(y_test_cat, y_pred_cat, output_dict=True)\n",
    "}\n",
    "\n",
    "# Greenwashing detection metrics\n",
    "greenwashing_metrics = {\n",
    "    'accuracy': (y_pred_gw == y_test_gw).mean(),\n",
    "    'precision': precision_score(y_test_gw, y_pred_gw),\n",
    "    'recall': recall_score(y_test_gw, y_pred_gw),\n",
    "    'f1_score': f1_score(y_test_gw, y_pred_gw),\n",
    "    'roc_auc': roc_auc_gw,\n",
    "    'cv_roc_auc_mean': cv_scores_gw.mean(),\n",
    "    'cv_roc_auc_std': cv_scores_gw.std(),\n",
    "    'classification_report': classification_report(y_test_gw, y_pred_gw, output_dict=True)\n",
    "}\n",
    "\n",
    "# Model comparison metrics\n",
    "comparison_metrics = {\n",
    "    'model_comparison': results,\n",
    "    'best_model': max(results.items(), key=lambda x: x[1]['roc_auc'])[0],\n",
    "    'best_roc_auc': max(results.items(), key=lambda x: x[1]['roc_auc'])[1]['roc_auc']\n",
    "}\n",
    "\n",
    "# Save all metrics\n",
    "baseline_metrics = {\n",
    "    'category_classification': category_metrics,\n",
    "    'greenwashing_detection': greenwashing_metrics,\n",
    "    'model_comparison': comparison_metrics,\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(df_model),\n",
    "        'training_samples': X_train_gw.shape[0],\n",
    "        'test_samples': X_test_gw.shape[0],\n",
    "        'greenwashing_rate': df_model['greenwashing_flag'].mean(),\n",
    "        'vocabulary_size': len(tfidf.vocabulary_)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../metrics/baseline_metrics.json', 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "print(\"Saved: baseline_metrics.json\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_data = {\n",
    "    'top_greenwashing_features': [\n",
    "        {'feature': feature_names[i], 'importance': float(feature_importance[i])} \n",
    "        for i in top_positive_idx\n",
    "    ],\n",
    "    'top_legitimate_features': [\n",
    "        {'feature': feature_names[i], 'importance': float(feature_importance[i])} \n",
    "        for i in top_negative_idx\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../metrics/feature_importance.json', 'w') as f:\n",
    "    json.dump(feature_importance_data, f, indent=2)\n",
    "print(\"Saved: feature_importance.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BASELINE MODELS SUMMARY ===\\n\")\n",
    "\n",
    "print(\"1. DATASET:\")\n",
    "print(f\"   - Total samples: {baseline_metrics['dataset_info']['total_samples']}\")\n",
    "print(f\"   - Training samples: {baseline_metrics['dataset_info']['training_samples']}\")\n",
    "print(f\"   - Test samples: {baseline_metrics['dataset_info']['test_samples']}\")\n",
    "print(f\"   - Greenwashing rate: {baseline_metrics['dataset_info']['greenwashing_rate']:.2%}\")\n",
    "print(f\"   - Vocabulary size: {baseline_metrics['dataset_info']['vocabulary_size']}\")\n",
    "\n",
    "print(\"\\n2. CATEGORY CLASSIFICATION:\")\n",
    "print(f\"   - Accuracy: {category_metrics['accuracy']:.3f}\")\n",
    "print(f\"   - CV Accuracy: {category_metrics['cv_accuracy_mean']:.3f} (+/- {category_metrics['cv_accuracy_std']*2:.3f})\")\n",
    "\n",
    "print(\"\\n3. GREENWASHING DETECTION:\")\n",
    "print(f\"   - Accuracy: {greenwashing_metrics['accuracy']:.3f}\")\n",
    "print(f\"   - Precision: {greenwashing_metrics['precision']:.3f}\")\n",
    "print(f\"   - Recall: {greenwashing_metrics['recall']:.3f}\")\n",
    "print(f\"   - F1-Score: {greenwashing_metrics['f1_score']:.3f}\")\n",
    "print(f\"   - ROC-AUC: {greenwashing_metrics['roc_auc']:.3f}\")\n",
    "print(f\"   - CV ROC-AUC: {greenwashing_metrics['cv_roc_auc_mean']:.3f} (+/- {greenwashing_metrics['cv_roc_auc_std']*2:.3f})\")\n",
    "\n",
    "print(\"\\n4. MODEL COMPARISON:\")\n",
    "print(f\"   - Best model: {comparison_metrics['best_model']}\")\n",
    "print(f\"   - Best ROC-AUC: {comparison_metrics['best_roc_auc']:.3f}\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS:\")\n",
    "print(\"   - TF-IDF + Logistic Regression provides solid baseline performance\")\n",
    "print(\"   - Model shows good ability to distinguish greenwashing from legitimate claims\")\n",
    "print(\"   - Feature importance reveals key terms associated with greenwashing\")\n",
    "print(\"   - Cross-validation confirms model stability\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS:\")\n",
    "print(\"   - Models saved and ready for deployment\")\n",
    "print(\"   - Proceed to notebook 04_model_tuning.ipynb for advanced models\")\n",
    "print(\"   - Consider ensemble methods and hyperparameter tuning\")\n",
    "print(\"   - Explore transformer-based models for better performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 